{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidancrilly/MiniCourse-DifferentiableSimulation/blob/main/02_DifferentiableSimulatorsAndOptimisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ol9R_T9VkAcs"
      },
      "source": [
        "# Interactive Exercise\n",
        "\n",
        "Resources:\n",
        "\n",
        "- JAX [documentation](https://jax.readthedocs.io/en/latest/quickstart.html)\n",
        "- Patrick Kidger \"On Neural Differential Equations\" [ArXiv link](https://arxiv.org/abs/2202.02435)\n",
        "\n",
        "Note: need to install diffrax and optax libraries as not installed by default on colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "um1XqNf7j3IM"
      },
      "outputs": [],
      "source": [
        "!pip install diffrax optax\n",
        "import diffrax\n",
        "import optax\n",
        "import matplotlib.pyplot as plt\n",
        "import jax.numpy as jnp\n",
        "import jax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8VpdkwHkVAj"
      },
      "source": [
        "# Part 1\n",
        "\n",
        "For the following exercises we are going to introduce two JAX libraries which will allow us to easily write and train differentiable simulators:\n",
        "\n",
        "1. diffrax - https://docs.kidger.site/diffrax/\n",
        "2. optax - https://optax.readthedocs.io/en/latest/getting_started.html\n",
        "\n",
        "These libraries allow for the numerical forward and adjoint solution to differential equations (diffrax) and the solving of optimisation problems (optax).\n",
        "\n",
        "First, we introduce diffrax. The key features of diffrax we need to use are:\n",
        "\n",
        "1. ODETerms, these wrap python functions of the form:\n",
        "```\n",
        "def dydt(t : float, y : JAX array, args : dict):\n",
        "    return dydt_val : JAX array\n",
        "```\n",
        "which return the right hand side of your differential equation.\n",
        "\n",
        "2. Solvers, these implement different numerical methods for evolving in time. For example, Euler's method is available as well as other higher order methods.\n",
        "\n",
        "3. Step size controllers, the solvers we will use are adaptive (they vary the time step used) so these controllers are used to decide what step size to take to obtain a prescribed accuracy\n",
        "\n",
        "4. SaveAts, these simply provide a number of points in time when the numerical solution should be saved and returned to the user.\n",
        "\n",
        "Using these features, diffrax numerically solves the following equation:\n",
        "\n",
        "$$\n",
        "y(t_1) = y(t_0) + \\int_{t_0}^{t_1} \\frac{dy}{dt} dt\n",
        "$$\n",
        "\n",
        "and thus we must also specify the time interval $[t_0,t_1]$ and the intitial conditions $y(t_0) \\equiv y_0$.\n",
        "\n",
        "## Part a: Solving differential equations in JAX\n",
        "\n",
        "We will first learn to use the key features of diffrax using the exponential decay ODE example from the previous exercise.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1LgCzFOfj7PF"
      },
      "outputs": [],
      "source": [
        "def dydt(t,y,args):\n",
        "  return -y/args['tau']\n",
        "\n",
        "def diffrax_solve(dydt,t0,t1,Nt,rtol=1e-5,atol=1e-5):\n",
        "  \"\"\"\n",
        "  Here we wrap the diffrax diffeqsolve function such that we can run with\n",
        "  different y0s and taus over the same time interval easily\n",
        "  \"\"\"\n",
        "  # We convert our python function to a diffrax ODETerm\n",
        "  term = diffrax.ODETerm(dydt)\n",
        "  # We chose a solver (time-stepping) method from within diffrax library\n",
        "  # Heun's method (https://en.wikipedia.org/wiki/Heun%27s_method)\n",
        "  solver = diffrax.Heun()\n",
        "\n",
        "  # At what time points you want to save the solution\n",
        "  saveat = diffrax.SaveAt(ts=jnp.linspace(t0,t1,Nt))\n",
        "  # Diffrax uses adaptive time stepping to gain accuracy within certain tolerances\n",
        "  stepsize_controller = diffrax.PIDController(rtol=rtol, atol=atol)\n",
        "\n",
        "  return lambda y0,tau : diffrax.diffeqsolve(term, solver,\n",
        "                         y0=y0, args = {'tau' : tau},\n",
        "                         t0=t0, t1=t1, dt0=(t1-t0)/Nt,\n",
        "                         saveat=saveat, stepsize_controller=stepsize_controller)\n",
        "\n",
        "t0 = 0.0\n",
        "t1 = 1.0\n",
        "Nt = 100\n",
        "\n",
        "ODE_solve = diffrax_solve(dydt,t0,t1,Nt)\n",
        "\n",
        "# Solve for specific y0 and tau\n",
        "y0 = 1.0\n",
        "tau = 0.5\n",
        "sol = ODE_solve(y0,tau)\n",
        "\n",
        "plt.plot(sol.ts,sol.ys)\n",
        "plt.plot(sol.ts,y0*jnp.exp(-sol.ts/tau),'k--')\n",
        "plt.legend(['numerical','exact'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkVeJhuBUNcv"
      },
      "source": [
        "Diffrax solutions are differentiable by construction - see https://docs.kidger.site/diffrax/api/adjoints/ for details.\n",
        "\n",
        "We can therefore very easily solve the adjoint state problem using diffrax and JAX AD:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vvRNDxjUNzv"
      },
      "outputs": [],
      "source": [
        "def loss(inputs):\n",
        "  y0 = inputs['y0']\n",
        "  tau = inputs['tau']\n",
        "  sol = ODE_solve(y0,tau)\n",
        "  return sol.ys[-1]\n",
        "\n",
        "inputs = {'y0' : y0, 'tau' : tau}\n",
        "# Returns gradient of loss with respect to all inputs, i.e. dLdtau and dLdy0\n",
        "jax.grad(loss)(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe0rEZ3EkZ4S"
      },
      "source": [
        "## Part b: Optimisation in JAX\n",
        "\n",
        "As means as of an introduction to optax, we will solve a very simple optimisation problem:\n",
        "\n",
        "$$\n",
        "\\mathrm{argmin}_p f(p) = \\underline{p}^T \\cdot \\underline{\\underline{d}} \\cdot \\underline{p}\n",
        "$$\n",
        "\n",
        "which is trivially solvable with $\\underline{p} = \\underline{0}$. We will also use this oppurtunity to make use of JAX's random number generators.\n",
        "\n",
        "Below is some code to set up the problem for randomly generated $\\underline{\\underline{d}}$ and starting location for the optimisation problem $\\underline{p}_0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_gkg-rckblS"
      },
      "outputs": [],
      "source": [
        "# RNG initialisation\n",
        "key = jax.random.key(0)\n",
        "\n",
        "def example_loss(p,d):\n",
        "  return jnp.dot(p.T,jnp.dot(d,p))\n",
        "\n",
        "# Dimension of input space\n",
        "Np = 10\n",
        "# Convex shape of input space\n",
        "d = jax.random.normal(key,shape=(Np,Np))\n",
        "# Random positive semi-definite\n",
        "d = jnp.dot(d,d.T)\n",
        "# Random starting location\n",
        "key, subkey = jax.random.split(key)\n",
        "p0 = jax.random.normal(subkey,shape=(Np,))\n",
        "print(f'Starting loss: {example_loss(p0,d)}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7o4Yl7qm4YG"
      },
      "source": [
        "Optax defines the optimisation workflow through two important components:\n",
        "\n",
        "1. The optimizer: this defines the optimizer algorithm and uses the gradients of the loss (combined with the optimizer hyperparameters) to update the trainable parameters\n",
        "2. The optimizer state: this defines the trainable parameters\n",
        "\n",
        "In the code below, we define these optax components for our simple convex optimisation problem. We also define the gradient of the loss via AD. Finally, we introduce a training loop which iteratively updates the parameters via the adam optimiser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o60-KSE9lTyF"
      },
      "outputs": [],
      "source": [
        "# Initialize parameters of the model + optimizer.\n",
        "learning_rate = 1e-1\n",
        "optimizer = optax.adam(learning_rate)\n",
        "opt_state = optimizer.init(p0)\n",
        "\n",
        "# A simple update loop\n",
        "Nepoch = 200\n",
        "grad_loss = jax.value_and_grad(example_loss)\n",
        "p = p0.copy()\n",
        "history = []\n",
        "for _ in range(Nepoch):\n",
        "  loss,grads = grad_loss(p, d)\n",
        "  # Optax optimizer uses the gradients to update the parameters and the optimiser state\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  p = optax.apply_updates(p, updates)\n",
        "  history.append(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwxAjoivm2GL"
      },
      "source": [
        "Plotting the loss history, we see it converging towards the global optimum at 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsXQ9YYymRst"
      },
      "outputs": [],
      "source": [
        "plt.semilogy(history)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "print(f'Final p mag: {jnp.linalg.norm(p)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMzJ6LnnrObF"
      },
      "source": [
        "# Neural Differential Equations\n",
        "\n",
        "In the most simple form, a neural differential equation (NDE) has the following structure:\n",
        "\n",
        "$$\n",
        "\\frac{d y}{d t} = \\mathcal{N}_\\theta(y,t)\n",
        "$$\n",
        "\n",
        "where $\\mathcal{N}_\\theta(y,t)$ is a neural network with parameters $\\theta$.\n",
        "\n",
        "The numerical solution can be computed using traditional techniques used for ODEs and PDEs. The adjoint solution can then be used to train the neural network to improve the solution. Due to the flexibility of neural networks, NDEs can describe a wide variety of systems in a time-continuous manner.\n",
        "\n",
        "In the following we will consider a simple example where we will use a NDE to describe the response of an electrical circuit for which we have collected data but do not know its internal structure.\n",
        "\n",
        "For neural networks in JAX we will use equinox:\n",
        "\n",
        "- equinox [documentation](https://docs.kidger.site/equinox/)\n",
        "\n",
        "and then diffrax and optax can be used in the same way as before to solve both the forward and adjoint problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ROV_o5tgvK42"
      },
      "outputs": [],
      "source": [
        "!pip install equinox\n",
        "import equinox as eqx\n",
        "import jax.nn as jnn\n",
        "import jax.random as jrandom"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46UbioTOvRxF"
      },
      "source": [
        "Given the theory of electical circuits, we propose a simple NDE of the form:\n",
        "\n",
        "$$ \\frac{d}{dt} \\begin{bmatrix}I \\\\ h\\end{bmatrix} = \\begin{bmatrix}h \\\\ \\mathcal{N}_\\theta(I,h)\\end{bmatrix} + \\begin{bmatrix}0 \\\\ \\frac{dV_{ext.}}{dt}\\end{bmatrix}$$\n",
        "\n",
        "Where $I$ is the electical current and $V_{ext.}$ is the externally applied voltage. We introduce a hidden state $h$ which linearly responds to the temporal gradient of the applied voltage. The neural network then computes the temporal response of the hidden state given the solution values at time $t$.\n",
        "\n",
        "First, we load in the training data, a data set of ($t$, $I$) for a oscillating applied voltage $V(t,\\omega,T)$ at various $\\omega$ values. The currents ($I$) have been corrupted by noise and therefore our NDE model must be robust to this noise to be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jDBzn7__yc5m"
      },
      "outputs": [],
      "source": [
        "T = 10.0 #s for all data sets\n",
        "\n",
        "# Upload the data provided in course git repo to the sample_data folder\n",
        "# <- Open the colab file browser by the folder icon of the left\n",
        "omegas = jnp.load('./sample_data/omegas.npy')\n",
        "ts = jnp.load('./sample_data/ts.npy')\n",
        "Is = jnp.load('./sample_data/Is.npy')\n",
        "\n",
        "# Save only one data set for testing in this simple example\n",
        "ts_train, Is_train, omegas_train = ts[:], Is[:-1,:], omegas[:-1]\n",
        "ts_test, Is_test, omegas_test    = ts[:], Is[-1:,:], omegas[-1:]\n",
        "\n",
        "# Applied voltage function\n",
        "def V(t,omega,T):\n",
        "    \"\"\"\n",
        "    Applied voltage function, frequency increases quadratically in time\n",
        "    \"\"\"\n",
        "    omega0 = omega*(2*t/T)\n",
        "    return jnp.sin(omega0*t)\n",
        "\n",
        "# To be completed\n",
        "# Create a gradient function of V, dVdt\n",
        "# Also create a vmapped version, which maps over the time dimension (see in_axes argument of jax.vmap)\n",
        "dVdt =\n",
        "dVdt_vmapped ="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55Xs2e8_741O"
      },
      "source": [
        "Lets quickly view the data and the forcing term (dV/dt):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nOpVy5_6iJF"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(dpi=100)\n",
        "ax1 = fig.add_subplot(211)\n",
        "ax2 = fig.add_subplot(212,sharex=ax1)\n",
        "ax1.plot(ts,Is.T,'k')\n",
        "for omega in omegas:\n",
        "  ax2.plot(ts,dVdt_vmapped(ts,omega,T),'r')\n",
        "ax1.set_xlim(ts[0],ts[-1])\n",
        "ax2.set_xlabel('t')\n",
        "ax1.set_ylabel('I')\n",
        "ax2.set_ylabel('dVdt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYLPAozfydZL"
      },
      "source": [
        "Next, we set up an equinox Module that contains an MultiLayer Perceptron (MLP) for $\\mathcal{N}_\\theta(I,h)$ and its call method evaluated the right hand side of the full NDE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8HUUDdprvj9"
      },
      "outputs": [],
      "source": [
        "# NDE solution\n",
        "# Following https://docs.kidger.site/diffrax/examples/neural_ode/\n",
        "class NeuralODE_RHS(eqx.Module):\n",
        "    mlp: eqx.nn.MLP\n",
        "\n",
        "    def __init__(self, in_size, out_size, width_size, depth, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp = eqx.nn.MLP(\n",
        "            in_size=in_size,\n",
        "            out_size=out_size,\n",
        "            width_size=width_size,\n",
        "            depth=depth,\n",
        "            activation=jnn.leaky_relu,\n",
        "            key=key,\n",
        "            use_bias=False,\n",
        "            use_final_bias=False\n",
        "        )\n",
        "\n",
        "    def __call__(self, t, y, args):\n",
        "        # To be completed\n",
        "        dhdt =\n",
        "        dIdt =\n",
        "        # Stack the temporal responses into dydt\n",
        "        dydt =\n",
        "        return dydt\n",
        "\n",
        "class NeuralODE(eqx.Module):\n",
        "    in_size: int\n",
        "    func: NeuralODE_RHS\n",
        "\n",
        "    def __init__(self, in_size, out_size, width_size, depth, *, key, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.in_size = in_size\n",
        "        self.func = NeuralODE_RHS(in_size, out_size, width_size, depth, key=key)\n",
        "\n",
        "    def __call__(self, ts, omega, args):\n",
        "        \"\"\"\n",
        "        Similar to our examples above, we set up diffeqsolve\n",
        "        but now our ODETerm uses our NeuralODE_RHS equinox Module.\n",
        "        \"\"\"\n",
        "        # Add relevant data to the args dictionary\n",
        "        args['omega'] = omega\n",
        "        solution = diffrax.diffeqsolve(\n",
        "            diffrax.ODETerm(self.func),\n",
        "            diffrax.Heun(),\n",
        "            t0=ts[0],\n",
        "            t1=ts[-1],\n",
        "            dt0=ts[1] - ts[0],\n",
        "            y0=jnp.zeros(self.in_size),\n",
        "            args=args,\n",
        "            stepsize_controller=diffrax.PIDController(rtol=1e-2, atol=1e-4),\n",
        "            saveat=diffrax.SaveAt(ts=ts),\n",
        "            max_steps=int(1e6)\n",
        "        )\n",
        "        return solution.ys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nH31C6o0-6m"
      },
      "source": [
        "Now we will train our NDE using the adjoint method and the available training data of ($t$,$I$,$\\omega$).\n",
        "\n",
        "The training process will look very similar to what you implemented above in the diffusion example. The only thing to notice is that break the learning up over increasingly long sections of the data. So we train on the first x time steps for n epochs and then the next y time steps for m epochs, etc.. This is a standard trick to avoid getting caught in a local minimum.\n",
        "\n",
        "There are a few lines to complete in order to run the training of the NDE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZntHwF9zt3x"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "def train_NDE(\n",
        "    ts,ys,omegas,args,\n",
        "    width_size,depth,\n",
        "    optimiser,\n",
        "    lr_strategy,\n",
        "    steps_strategy,\n",
        "    length_strategy,\n",
        "    seed=420,\n",
        "    print_every=50\n",
        "):\n",
        "    \"\"\"\n",
        "    ts : jax array of data times\n",
        "    ys : jax array of data currents\n",
        "    omegas : jax array of data drive frequencies\n",
        "    width_size : number of neurons in hidden layers\n",
        "    depth : number of hidden layers\n",
        "    optimiser : optax optimiser to be used\n",
        "    lr_strategy : learning rate schedule in tuple\n",
        "    steps_strategy : number of training steps in tuple\n",
        "    length_strategy : fraction of training data used in training in tuple\n",
        "    seed : PRNG seed value\n",
        "    print_every : print every n steps of training\n",
        "    \"\"\"\n",
        "    key = jrandom.PRNGKey(seed)\n",
        "    __, model_key = jrandom.split(key)\n",
        "\n",
        "    length_size = ts.shape[0]\n",
        "\n",
        "    # To be completed, what is the shape of our input and output layer\n",
        "    in_size =\n",
        "    out_size =\n",
        "\n",
        "    model = NeuralODE(in_size, out_size, width_size, depth, key=model_key)\n",
        "\n",
        "    @eqx.filter_value_and_grad\n",
        "    def grad_loss(model, ti, yi, omegas):\n",
        "        # Handle batch\n",
        "        batched_model = jax.vmap(model,in_axes=(None,0,None))\n",
        "        y_pred = batched_model(ti, omegas, args)\n",
        "        # Only compute the MSE using the NDE current (I) prediction\n",
        "        # To be completed\n",
        "        MSE =\n",
        "        return MSE\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def make_step(ti, yi, omegas, model, opt_state):\n",
        "        loss, grads = grad_loss(model, ti, yi, omegas)\n",
        "        updates, opt_state = optim.update(grads, opt_state)\n",
        "        model = eqx.apply_updates(model, updates)\n",
        "        return loss, model, opt_state\n",
        "\n",
        "    count = 0\n",
        "    for lr, steps, length in zip(lr_strategy, steps_strategy, length_strategy):\n",
        "        optim = optimiser(lr)\n",
        "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "        _ts = ts[: int(length_size * length)]\n",
        "        _ys = ys[:, : int(length_size * length)]\n",
        "        for step in range(steps):\n",
        "            count += 1\n",
        "            start = time.time()\n",
        "            loss, model, opt_state = make_step(_ts, _ys, omegas, model, opt_state)\n",
        "            end = time.time()\n",
        "            if (step % print_every) == 0 or step == steps - 1:\n",
        "                print(f\"Step: {step}, Loss: {loss}, Computation time: {end - start}\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f96niUdKwpT"
      },
      "source": [
        "We will make a very small neural network model in this case, both for quick training and for regularisation such that we do not overfit the noisy data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idCQcOf1-eBM"
      },
      "outputs": [],
      "source": [
        "# MLP hyperparameters\n",
        "width_size = 4  # Number of neurons in hidden layers\n",
        "depth = 2       # Number of hidden layers (including output layer)\n",
        "\n",
        "optimiser = optax.adabelief\n",
        "\n",
        "NDE_args = {'dVdt' : dVdt, 'T' : T}\n",
        "\n",
        "# Training should take just a 1-2 mins\n",
        "trained_model = train_NDE(ts_train,Is_train,omegas_train,\n",
        "                          NDE_args,\n",
        "                          width_size,depth,\n",
        "                          optimiser,\n",
        "                          lr_strategy=(1e-2,1e-2,1e-3),\n",
        "                          steps_strategy=(200,200,400),\n",
        "                          length_strategy=(0.5,1.0,1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vglsERoj1kXY"
      },
      "source": [
        "Now, we have trained our NDE we can test its results at an unseen value of $\\omega$.\n",
        "\n",
        "Below, write code to compute the NDE solution at all omegas (including the test data) and compare the predicted and true current values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQudRVjez4mq"
      },
      "outputs": [],
      "source": [
        "# Only omega changes between data samples\n",
        "compute_trained_model_pred = lambda omega : trained_model(ts, omega, NDE_args)\n",
        "\n",
        "# Plot trained model against train and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wclo1eW27jb"
      },
      "source": [
        "With the tuned hyperparameters above, this NDE can capture the behaviour of the electrical circuit with high accuracy on both training and test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Physics-Informed Neural Network\n",
        "\n",
        "The following demonstrates the use of a Physics-Informed Neural Network (PINN) to solve the 1D diffusion equation: \n",
        "\n",
        "$$ \\frac{\\partial u}{\\partial t} = D \\frac{\\partial^2 u}{\\partial x^2} $$\n",
        "\n",
        "This is a common example in physics and engineering, and PINNs provide a way to solve such differential equations using neural networks, by encoding the equation itself into the loss function of the network.\n",
        "\n",
        "Key concepts demonstrated:\n",
        "- A neural network is used to approximate the solution u(x, t).\n",
        "- The loss function is composed of two parts:\n",
        "    1. Mean Squared Error (MSE) loss: This ensures the solution fits any available \"ground truth\" data points.\n",
        "    2. Physics loss: This ensures the solution obeys the diffusion equation. This loss is calculated from the residual of the PDE.\n",
        "- Automatic differentiation (as provided by JAX) is used to compute the derivatives of the neural network's output with respect to its inputs (x and t),which is essential for calculating the physics loss.\n",
        "\n",
        "\n",
        "First, we define the analytic diffusion solution from which we will draw training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Diffusivity constant for the diffusion equation.\n",
        "D = 1.0\n",
        "\n",
        "def diffusion_solution(t, x):\n",
        "    \"\"\"\n",
        "    Analytical solution to the 1D diffusion equation for a Dirac delta\n",
        "    function initial condition at t=0. This serves as our ground truth for\n",
        "    generating training data and for comparison.\n",
        "\n",
        "    Args:\n",
        "        t (jax.numpy.ndarray): Time coordinates.\n",
        "        x (jax.numpy.ndarray): Spatial coordinates.\n",
        "\n",
        "    Returns:\n",
        "        jax.numpy.ndarray: The value of the solution u(x, t).\n",
        "    \"\"\"\n",
        "    return jnp.exp(-x**2 / (4 * D * t)) / jnp.sqrt(4 * jnp.pi * D * t)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we define our PINN with a MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PINN(eqx.Module):\n",
        "    \"\"\"\n",
        "    The Physics-Informed Neural Network model.\n",
        "\n",
        "    This is a simple Multi-Layer Perceptron (MLP) that takes spatial (x) and\n",
        "    temporal (t) coordinates as input and outputs the predicted value of the\n",
        "    solution u(x, t).\n",
        "    \"\"\"\n",
        "    mlp: eqx.nn.MLP\n",
        "\n",
        "    def __init__(self, in_size, out_size, width_size, depth, *, key, **kwargs):\n",
        "        \"\"\"\n",
        "        Initializes the MLP.\n",
        "\n",
        "        Args:\n",
        "            in_size (int): Input size (2 for x and t).\n",
        "            out_size (int): Output size (1 for u).\n",
        "            width_size (int): Number of neurons in each hidden layer.\n",
        "            depth (int): Number of hidden layers.\n",
        "            key (jax.random.PRNGKey): JAX random key for initialization.\n",
        "        \"\"\"\n",
        "        super().__init__(**kwargs)\n",
        "        self.mlp = eqx.nn.MLP(\n",
        "            in_size=in_size,\n",
        "            out_size=out_size,\n",
        "            width_size=width_size,\n",
        "            depth=depth,\n",
        "            activation=jnn.tanh,\n",
        "            key=key,\n",
        "        )\n",
        "\n",
        "    def __call__(self, x, t):\n",
        "        \"\"\"\n",
        "        Performs a forward pass of the network.\n",
        "\n",
        "        Args:\n",
        "            x (float): Spatial coordinate.\n",
        "            t (float): Temporal coordinate.\n",
        "\n",
        "        Returns:\n",
        "            float: The predicted value of the solution u(x, t).\n",
        "        \"\"\"\n",
        "        # The inputs are concatenated to form a single input vector for the MLP.\n",
        "        input_vec = jnp.array([x, t])\n",
        "        y_PINN = self.mlp(input_vec)\n",
        "        return y_PINN.reshape(())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we define our training function which includes our definition of both the loss terms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_PINN(\n",
        "    training_ts, training_xs, training_sol,\n",
        "    lr_strategy=(1e-3,),\n",
        "    steps_strategy=(2000,),\n",
        "    width_size=32,\n",
        "    depth=3,\n",
        "    seed=5678,\n",
        "    print_every=50,\n",
        "):\n",
        "    \"\"\"\n",
        "    Trains the PINN model.\n",
        "\n",
        "    Args:\n",
        "        training_ts (jax.numpy.ndarray): Time coordinates for training data.\n",
        "        training_xs (jax.numpy.ndarray): Spatial coordinates for training data.\n",
        "        training_sol (jax.numpy.ndarray): Solution values at training points.\n",
        "        lr_strategy (tuple): Tuple of learning rates for the optimizer.\n",
        "        steps_strategy (tuple): Tuple of number of training steps for each learning rate.\n",
        "        width_size (int): Width of the neural network.\n",
        "        depth (int): Depth of the neural network.\n",
        "        seed (int): Random seed.\n",
        "        print_every (int): How often to print loss values.\n",
        "\n",
        "    Returns:\n",
        "        PINN: The trained model.\n",
        "    \"\"\"\n",
        "    key = jrandom.PRNGKey(seed)\n",
        "    __, model_key = jrandom.split(key)\n",
        "\n",
        "    in_size = \n",
        "    out_size =\n",
        "    model = PINN(in_size, out_size, width_size, depth, key=model_key)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def PDE_loss(model, ti, xi):\n",
        "        \"\"\"\n",
        "        Calculates the physics-based loss.\n",
        "\n",
        "        This function computes the residual of the diffusion equation.\n",
        "        The goal of the training is to minimize this residual, effectively\n",
        "        forcing the neural network to satisfy the physics of the problem.\n",
        "        \"\"\"\n",
        "        # Use jax.grad to compute the derivatives of the model's output.\n",
        "        # `jax.vmap` is used to apply the function over the batch of inputs.\n",
        "\n",
        "        # ∂²u/∂x²\n",
        "        d2ydx2 = jax.vmap(jax.grad(jax.grad(model, argnums=0), argnums=0))\n",
        "        # ∂u/∂t\n",
        "        dydt = \n",
        "\n",
        "        # The residual of the PDE: ∂u/∂t - D * ∂²u/∂x²\n",
        "        # For a perfect solution, this would be zero.\n",
        "        g_PDE = dydt(xi, ti) - D * d2ydx2(xi, ti)\n",
        "\n",
        "        # We return the mean squared residual.\n",
        "        return jnp.mean(g_PDE**2)\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def MSE_loss(model, ti, xi, yi):\n",
        "        \"\"\"\n",
        "        Calculates the Mean Squared Error (MSE) loss.\n",
        "\n",
        "        This is the \"data-driven\" part of the loss. It measures how well the\n",
        "        network's prediction matches the provided training data.\n",
        "        \"\"\"\n",
        "        y_pred = jax.vmap(model)(xi, ti)\n",
        "        MSE = \n",
        "        return MSE\n",
        "\n",
        "    @eqx.filter_value_and_grad\n",
        "    def grad_loss(model, ti, xi, yi):\n",
        "        \"\"\"\n",
        "        Calculates the total loss and its gradient.\n",
        "\n",
        "        The total loss is a sum of the MSE loss and the PDE loss. This is the\n",
        "        core idea of a PINN: the model learns to satisfy both the data and the\n",
        "        underlying physics simultaneously.\n",
        "        \"\"\"\n",
        "        mse_loss = MSE_loss(model, ti, xi, yi)\n",
        "        pde_loss = PDE_loss(model, ti, xi)\n",
        "        return mse_loss + pde_loss\n",
        "\n",
        "    @eqx.filter_jit\n",
        "    def make_step(ti, xi, yi, model, opt_state):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step.\n",
        "        \"\"\"\n",
        "        loss, grads = grad_loss(model, ti, xi, yi)\n",
        "        updates, opt_state = optim.update(grads, opt_state)\n",
        "        model = eqx.apply_updates(model, updates)\n",
        "        return loss, model, opt_state\n",
        "\n",
        "    # Training loop\n",
        "    history = []\n",
        "    count = 0\n",
        "    for lr, steps in zip(lr_strategy, steps_strategy):\n",
        "        optim = optax.adabelief(lr)\n",
        "        opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
        "        for step in range(steps):\n",
        "            count += 1\n",
        "            loss, model, opt_state = make_step(training_ts, training_xs, training_sol, model, opt_state)\n",
        "            history.append(loss)\n",
        "\n",
        "            if (step % print_every) == 0 or step == steps - 1:\n",
        "                print(f\"Step: {step}, MSE Loss: {MSE_loss(model, training_ts, training_xs, training_sol)}, PDE Loss: {PDE_loss(model, training_ts, training_xs)}\")\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Running the code below with initialise and train our PINN on data pulled from the analytic solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define the spatial and temporal domain for plotting.\n",
        "extent = 4.0\n",
        "\n",
        "# Generate training data.\n",
        "# We create a set of random points in space and time and use the analytical\n",
        "# solution to get the \"ground truth\" values at these points.\n",
        "data_seed = 404\n",
        "key = jrandom.PRNGKey(data_seed)\n",
        "Ntrain = 100\n",
        "\n",
        "training_xs = jax.random.uniform(key, shape=(Ntrain,), minval=-extent, maxval=extent)\n",
        "__, key = jrandom.split(key)\n",
        "training_ts = jax.random.uniform(key, shape=(Ntrain,), minval=0.5, maxval=1.5)\n",
        "\n",
        "# Get the solution at the training points.\n",
        "training_ys = diffusion_solution(training_ts, training_xs)\n",
        "\n",
        "# Train the PINN model.\n",
        "NDE_model, history = train_PINN(training_ts, training_xs, training_ys)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training history\n",
        "fig = plt.figure(dpi=100)\n",
        "plt.semilogy(history)\n",
        "plt.xlabel('Training Step')\n",
        "plt.ylabel('Total Loss')\n",
        "plt.title('Training History of PINN')\n",
        "\n",
        "# Plot solution\n",
        "plot_ts = jnp.linspace(0.01, 1.5, 100)\n",
        "plot_xs = jnp.linspace(-extent, extent, 200)\n",
        "plot_Ts, plot_Xs = jnp.meshgrid(plot_ts, plot_xs)\n",
        "plot_sol = diffusion_solution(plot_Ts, plot_Xs)\n",
        "plot_PINN_sol = jax.vmap(NDE_model)(plot_Xs.flatten(), plot_Ts.flatten()).reshape(plot_Xs.shape)\n",
        "\n",
        "fig = plt.figure(dpi=100)\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax2 = fig.add_subplot(122)\n",
        "\n",
        "c1 = ax1.pcolormesh(plot_Ts, plot_Xs, plot_sol, shading='auto')\n",
        "fig.colorbar(c1, ax=ax1)\n",
        "c2 = ax2.pcolormesh(plot_Ts, plot_Xs, plot_PINN_sol, shading='auto')\n",
        "fig.colorbar(c2, ax=ax2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLsuVGWjy6It"
      },
      "source": [
        "# Key takeaways\n",
        "\n",
        "- Differentiable simulators require the following components:\n",
        "  1. A pre-defined ODE or PDE structure\n",
        "  2. A number of trainable parameters which are not a-priori known\n",
        "  3. A numerical scheme for solution (finite differencing, time-stepping, etc.)\n",
        "  4. A loss function which defines an optimal model\n",
        "  5. A means to minimise the loss (adjoint solver, optimiser, etc.)\n",
        "- Diffrax is a JAX library for the numerical solution (both forward and adjoint) of differential equations\n",
        "- Optax is a JAX library for optimisation which can be easily interfaced with diffrax\n",
        "- One can construct differential equations which include neural networks. Equinox is a JAX library which can be used for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uR5y3pMA2CjK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPKpG2EfXWupUkv4BcVcQC1",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
